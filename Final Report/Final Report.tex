\documentclass{article}

\usepackage[left=2.54cm,bottom=2.54cm,top=2.54cm,right=2.54cm,nohead,nofoot]{geometry}
\usepackage{listings}
\usepackage{graphicx,epsfig,latexsym,subfig}
\usepackage{amsmath,amsthm,amsfonts,amscd,amssymb,bm,mathtools,setspace,fancyhdr,commath,mathrsfs,mathtools,dsfont,tikz-cd,soul,booktabs,longtable,diagbox}
\usepackage{tikz}
\usepackage{tkz-euclide}
\usetkzobj{all}
\usetikzlibrary{calc,decorations.markings}
\everymath{\displaystyle}
\usepackage[toc,page]{appendix}
\usepackage{spverbatim}
\usepackage{float}

\pagestyle{empty}
\renewcommand{\headrulewidth}{2pt}
\cfoot{\thepage}
\cfoot{\vspace{0.4cm} \thepage}

\title{Santander Product Recommendation}

\begin{document}
	\maketitle
	\begin{center}\begin{Large} Xinyi Hou,
			Yingxi Yu,
			Yuan Xu,
			Wenyu Li,
			Qiaojuan Niu,
			Huiyu Bi,
			Meng Li,
			Zhongyu Fan,
			Aoran Zhang,
			Yuan Tian,
			Miao Wang,
			Markham Anderson,
			Bowen He,
			Charlton Lin,
			Zoran Dabic \end{Large}\end{center}
	\begin{spacing}{1}
		\begin{large}

\begin{abstract}
	 Our topic is specified in the Santander product recommendation competition on Kaggle. In detail, our goal is to predict which products the customer will purchase in the next month based on their past 1.5 year of behavioral and demographic data. To solve this problem, we use four different machine learning methods: logistic regression, SVM, random forest and XGBoost. Since the data size is large and there is no clear response variable in the training set, different subteams use different subsets of the original training set and build their own response variable. Thus,it is difficult to compare the four different methods. The logistic regression and SVM subteams both build 24 classifiers for 24 products and the accuracy is high, which is above 0.9. However, we cannot say these two classifiers are powerful, because the training set is quite unbalanced. The random forest and XGboost both build only one multi-classifier model. The accuracy for these two are 0.90 and 0.63. As these two subteams use different data cleaning method and response variable, we cannot say random forest is better than XGboost. In summary, all four methods could provide reasonable predictions and this would help the Santander Bank to build a more effective recommendation system.
\end{abstract}

\section{Introduction}
The project is aimed at building a more effective recommendation system. Through this system, Santander can predict which products their existing customers will buy in the next month based on their past behavior and that of similar customers. One of the difficulties is how to deal with the large data set. Indeed, the original data set contains 13 million records from January 2015 to June 2016, with approximate 930 thousand customers. How to deal with the large data set and transform the data to the model are the core problems of this project.

In our project, we applied four methods to solve this prediction problem: logistic regression, random forest, SVM and xgboost. For each method, the ways to clean and process the data are different but there are still some similarity.

This paper introduces four machine learning methods. In each sub parts, we will introduce the way we clean the data, models and results, and the discussions of this method.

\section{XGBoost}

\subsection{Methods}

\noindent \indent XGBoost is short for ``Extreme Gradient Boosting" and is widely used in machine learning problems, especially classification problems\cite{xg1}\cite{xg2}. 

\subsection{Data Pre-processing}

\noindent \indent Since there are many missing values and outliers, the first thing is to clean data. 27334 records with 9 consecutive NA values are deleted and the extreme values (for an instance, the 99 percent quantile of the age variable is 88 years old) are excluded. After exploring the original data, we decide to use 32 features to predict the response. The 32 features include \verb|"pais_residencia"|, \verb|"age"|, \verb|"ind_nuevo"|, \verb|"antiguedad"|, \verb|"ind_empleado"|, \verb|"renta"|, \verb|"ind_actividad_cliente"|, \verb|"segmento"| and 24 product status. For the \verb|"age"|, \verb|"antiguedad"| and \verb|"renta"|, we cut them into 5 intervals and they also become categorical features now.

The response is the additional product which the customer would add in the next month. If the customer adds more than one product, we only randomly pick one of them. To make the data size smaller and the problem easier, we subset the samples with nonzero response, which means we only consider the records with additional product.

\subsection{XGBoost Algorithm}

\noindent \indent To find the best parameters given the training data, we need to define a so-called objective function. Here for XGBoost algorithm, the objective function contains two parts: training loss and regularization.
\[
Obj(\Theta) = L(\theta) + \Omega(\theta)
\]
where $L$ is the training loss function, and $\Omega$ is the regularization term. The training loss measures how predictive our model is on training data. The regularization term controls the complexity of the model, which helps us to avoid overfitting. Here we define these two items as:
\[
L(\theta) = \sum_{i}(y_i - \hat{y_i})^2
\]
\[
\Omega(f) = \gamma T + \frac{1}{2}\lambda\sum_{j = 1}^{T}(w_j)^2
\]




\subsection{Results}

\noindent \indent Different from some other groups, we only apply the algorithm once and fit a multi-classifier model. We randomly pick 70\% of the data to be the training data, and the rest 30\% to be the test set. By changing the main parameters \cite{xg3} max.depth among 5, 6, 7, the number of trees among 10, 50, 100, and the step size among 0.1, 0.5, 1, we get the results displayed in the tables as below.
\begin{table}[h]
\centering
\caption{Accuracy for Training Data}
\label{Accuracy for Training Data}
\begin{tabular}{cccccccccc}
\hline
max.depth       & \multicolumn{3}{c}{5} & \multicolumn{3}{c}{6} & \multicolumn{3}{c}{7} \\ \hline
\# of trees     & 10    & 50    & 100   & 10  & 50       & 100  & 10    & 50    & 100   \\
step size = 0.1 & 62.06\%    & 62.45\%     & 62.79\%     & 62.32\%    & 62.77\%         & 63.10\%      & 62.54\%     & 62.89\%      & 63.24\%      \\
step size = 0.5 & 62.47\%    & 63.37\%       & 63.46\%      & 62.79\%   & 63.70\%  & 64.48\%     & 62.97\%       & 64.20\%      & 64.32\%      \\
step size = 1   &38.30\%       & 31.70\%      &35.07\%       &18.31\%     & 36.78\%         &36.65\%      &30.46\%       &46.37\%       &44.31\%       \\ \hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Accuracy for Test Data}
\label{Accuracy for Test Data}
\begin{tabular}{cccccccccc}
\hline
max.depth       & \multicolumn{3}{c}{5} & \multicolumn{3}{c}{6} & \multicolumn{3}{c}{7} \\ \hline
\# of trees     & 10    & 50    & 100   & 10  & 50       & 100  & 10    & 50    & 100   \\
step size = 0.1 & 62.09\%      & 62.37\%     & 62.63\%      &62.30\%     & 62.59\%       & 62.80\%      & 62.45\%      & 62.76\%      & 63.10\%      \\
step size = 0.5 & 62.41\%     & 62.80\%     & 62.92\%      & 62.57\%    &62.85\%     & 62.94\%   & 62.61\%      & 62.88\%      & 62.89\%      \\
step size = 1   &38.45\%       & 31.80\%      & 35.29\%      &18.30\%     & 36.72\%         &  36.98\% &30.61\%       &46.55\%      &44.45\%     \\ \hline
\end{tabular}
\end{table}

\noindent \indent From the above tables, we can see the highest accuracy for the training set is 64.48\%, and for the test set is 62.94\%, when max.depth = 6, the number of rounds(the number of trees) = 100 and the step size = 0.5.  When step size = 1, the accuracy for training and test set are about 40\%, which means this situation is not ideal.

\noindent \indent According to the best parameters that lead to highest test accuracy, we got the feature importance plot  as below. The feature importance figure shows the different gains for different features. From the plot, we can see features \verb|"ind_cno_fin_ult1"|, \verb|"ind_recibo_ult1"| and \verb|"ind_cco_fin_ult1"| obtain the first three highest gains.

\begin{figure}[H]
\centering
\includegraphics[width = 160mm]{113.png}
\caption{Feature Importance}
\end{figure}

\subsection{Discussion}

To make the model better, we could dive deeper. For example, we may consider more features and not cut some variables into intervals. We may also take the history length of the used products into account.

Due to the vague description of the problem, we are not sure how many new products to predict for each user. In fact, most members seldom tried new products based on historical records. Some valuable information might be lost since we drop some predictors and only pick one new product every time (if more than one).


\subsection{Author Contribution}

This subgroup consists of fours members, Xinyi Hou, Yingxi Yu, Wenyu Li, and Yuan Xu. Everyone in this subgroup gathered together to discuss how to tackle the problem, determine the method and tune parameters. More specifically, Xinyi was the leader of the whole big project group, pre-processed the data and wrote the abstract of the overall report; Yingxi wrote the initial model code and made the popwerpoint; Wenyu participated in initial model code writing and completed the report; and Yuan mainly focused on the report.

\section{Random Forest}

\subsection{Data Analysis}

We transform the data from time series to stationary data using a flattening process. Random forests do not handle time series very well. Had we not transformed the data we would have had to worry about how the training and testing subsets were selected to ensure that future data is not used to predict past data. In addition, a decision tree cannot split on a non repeating independent variable such as time to produce any useful results.

See \textbf{Appendix \ref{appendix:rn_data}} for what columns are removed, and how other columns are transformed.


\subsection{Testing}
Since the Kaggle provided test data does not include the true classifications, we flatten the training set on all months save for the last month: 2016-5. The product classifications from 2015-1 to 2016-4 is used as our true predictor variables for each customer.

\subsection{Method}

\paragraph{Random Forest}
We use a random forest model with 100 trees. A RF model uses decision trees as weak predictors of the response variable to produce a strong predictor in the random forest. Each tree is produced on a random subset of samples with replacement and would otherwise suffer from over fitting and high variance. At each split a random subset of features without replacement is used to find the best split. Each tree produces a classification and the mode of the classifications becomes the forest's overall classification for the sample. However for our model, sklearn instead averages the probability of predicting a class across all trees.\cite{rnSKlearn}  The out of bag samples for each tree is used to test the tree and provide an unbiased estimator of the classification error.\cite{rnLeo}

\paragraph{The Decision Tree}
Each split is made with respect to a random subset of features. The optimal split is chosen to minimize the impurity of the node being split on. The Gini impurity function is a common function to use. In all cases the goal is to split such that samples are grouped together by label.

\paragraph{Choosing the Random Forest}

Random forests scale very well with large input sizes. The bagging process results in a strong predictor over all the trees. The random forest can also handle our categorical variables directly so we will not have to generate dummy variables for the categories. Additionally, the random forest supports multi-label classification so we don't have to separately fit model for each response variable.

\subsection{Results}

\subsection{Forest Tuning}

We tested the forest with varying number of trees and maximum depth. As expected, accuracy increased with tree size and depth. Ultimately we chose a tree size of 100 and maximum depth of 100.
\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		\backslashbox{Trees}{Depth} & 10 & 100 & $\infty$ \\
		\hline
		1 & 0.7434 & 0.8594  & 0.8594 \\
		10 & 0.8465 & 0.8956  & 0.8956 \\
		100 & 0.8518 & 0.9044  & 0.8921 \\
		\hline
	\end{tabular}
	\caption{Model Accuracy vs \# of Trees and Tree Depth}
	\label{tab:my_label}
\end{table}


\subsection{ROC for 100 trees with depth of 10}
\begin{center}
	\includegraphics[width = 15cm]{ROC.png}
\end{center}

\subsection{Discussion}

As expected the benefits of the random forest can be seen in how our classification accuracy grows with the number of trees and tree depth. The very high accuracy for the forest of tree size one is most likely because most customers did not buy new products.


\subsection{Credits}

Charlton Lin: Created the flattening code and worked on feature engineering.
Qiaojuan Niu : Created the Random Forest model and code to clean the data.
Zoran Dabic: Wrote the subgroup report, compiled together the full report, worked on feature engineering, and ran the flatten and clean code to generate the final data sets.

\subsection{Reproduction}

Install Jupyter with the latest version of python. \\
Download the training and testing data from the Santander competition page. \\
Run fixData.py in the same directory as the training and testing data. This will generate fixed versions of the data sets. \\
Run flattenClean.py to create flattened versions of the fixed data sets. \\
Run the jupyter project RanForestForFlat.ipynb \\

\section{SVM}

\subsection{Data Processing}
\begin{enumerate}
	\item Independent Variable Selection

	In order to increase the accuracy of model and efficiency of calculation, remove should remove some independent variables:

	\begin{itemize}
		\item Since we should transform categorical variables into dummy variables, variables(canal\_ entrada) with too many levels would sharply increase the model complexity and decrease the effectiveness of calculation.

		\item We also leave out independent variables that have repeated information. For example, indresi(whether residence country is the same as the bank country) and nomprov(i.e Province name) reflect the similar information of the customer so we could only keep one of them.

		\item For the independent variables predominated by missing values,such as conyuemp, we leave them out because they could not reflect useful information.
	\end{itemize}

	According to these three  rules, we remove : pais\_residencia, fecha\_alta, indext, conyuemp, canal\_entrada, tipodom, cod\_prov and nomprov.

	\item  Missing values

	For the cases have missing values in any of the 24 dependent variables, we remove them for the reason that these missing values only proportion less than 1\% and did not show no obvious pattern.

	As for independent variables, we only find missing values in renta(gross income), we use the regional average to replace the missing values.

	\item Data Transformation

	For independent variables, we transform the categorical variables into dummy variable. For the variable with n levels, we create (n-1) dummy variables. Besides, aimed to avoid influence of units and scales, we standardize the continuous variables.

	Then we process the response variables.  Owing that our goal is to predict what additional products a customer will get in addition to what they already have at the last month. So the value of the next month minus that of the current month.  If the result is “1”, we could figure out the customer bought certain new product i. Otherwise The consumer did not buy new products.

	Thus, we could build model to predict additional products consumers would buy for next month given their personal characteristics and their consumption records at current month.
\end{enumerate}

\subsection{SVM Model}

Support vector machine (i.e SVM) is the supervised learning model that analyze the data used for classification. With associated learning algorithms, a line or a hyper-plane is found to classify the data. In addition to doing linear classification, SVM can also efficiently perform a non-linear classification using kernel, making the separating hyper-plane complicated and do the classification better.

In our case, we want the classifier to be less sensitive to outliers to keep the margin sufficiently large. To do so, we introduce “slack variables” that allow samples to be misclassified or be within the margin. Additionally, to make the samples more separable, we introduce the idea of kernel. Consequently, we have :

Decision function:
$$f(x) = \displaystyle\sum_{i}\alpha_i\Phi(x_i)\Phi(x)+b = \displaystyle\sum_{i}\alpha_{i} K(x_i, x)+b$$, where $K(x_i, x)$ is the Kernel function.

Dual formulation:
$$min P(w,b) = \frac{1}{2}(||\displaystyle\sum_{i=1}^{m}\alpha_i\Phi(x_i)||)^2 + C\displaystyle\sum_{i}H_{i}[y_{i}f(x_i)]$$

\subsection{Results}

Based on the result of SVM and real labels of the 24 dependent variables, we calculate accuracy for each estimation. Namely:
$$Accuracy = \frac{TP+TN}{P+N}$$
Where TP is count of true positive , TN is count of true negative , N is count of negative and P is count of positive.

We found that, for each of the 24 estimation, the accuracy is higher than 99\% which indicates a high performance of the model in predicting each single dependent variable.

The we calculate the overall accuracy:
$$Overall\enspace Accuracy = \frac{Counts\enspace of\enspace the\enspace cases\enspace which\enspace all\enspace the\enspace 24\enspace variables\enspace are\enspace predicted\enspace correctly}{total\enspace number\enspace of\enspace cases}$$
In this case, the overall accuracy rate is 98\% which shows SVM method accuate result.

\subsection{Discussion}

However, scuritiny of the data reveals simply focusing on the accuracy is not reasonable. Precisely, for most dependent variables, the negative class 0 is predominant (typically greater than 99\%). Thus, even though all the predictions are zeros, the accuracy would be still easily over 99\%, based on which, the high performance might be caused by the fact that wrong prediction in 1s does not have much influence on the accuracy. To examine this idea, we choose 3 different dependent variables, each of which has very asymmetric distribution in 0s and 1s, and then use ROC and PRC to exhibit the results:

$$True\enspace Positive\enspace Rate = Recall = \frac{TP}{TP+FN}, Precision = \frac{TP}{TP+FP}$$

In the following plots, we can observe when the negative class 0 is highly predominant, the ROC and PRC suggest our results are actually problematic despite the fact that the accuracys are pretty good. Below is the confusion matrix for response variable "ind\_cco\_fin\_ult1".

\vspace{5mm}
\begin{tabular}{| l | l | l |}
	\hline
	& Really True & Really False \\ \hline
	Predicted True & 0 & 0 \\ \hline
	Predicted False & 558 & 99442\\
	\hline
\end{tabular}
\vspace{5mm}

The accuracy here is really high only, which is 99.44\%. However the model is completely useless since we are very interested in helping the bank to recommend products to a potential customer in addition to what he/she already has. Thus, it is very important to correctly classify 1 as 1. Failing to consider other metrics of evaluation like precision and recall would lead to an unjustified model. Here the recall is 0 and the precision is NA because there is not a single observation being classified as 1. The reason for this is that the negative class 0 is predominant in "ind\_cco\_fin\_ult1" and the model tends to classify everything as 0, producing that all the 1’s are classified as 0. The results are even worse for the other two variables as the class 1 are fewer.

ROC and PRC curve for "ind\_cco\_fin\_ult1" , "ind\_fond\_fin\_ult1" and "ind\_deme\_fin\_ult1"

\vspace{5mm}
\includegraphics[scale = 0.8]{ROCandPR.png}
\vspace{5mm}

We come up with two solutions to deal with this skewed class problem. One is to replace random sampling for training set and testing data by stratified sampling, which assures that the training set and testing data are well balanced. Random sampling seems to be reasonable but in our case, class 1 is really scarce. Therefore it is likely that all those class 1 being sampled into testing set, which makes it impossible for our algorithm to learn the hidden pattern of class 1. The other workaround is to add higher weights to the minority class which would encourage the model to be gear towards classifying 1 as 1 and add some penalty for model to predict 0. We tried the second option by adding weights to the two classes which are inverse proportional to their fractions. The results are as follows. It is shown that all class 1’s are correctly classified! Now the precision and recall are 1.64\% and 100\%. But there is some inevitable trade offs as in general one of these two metric gets better, the other will get worse. Additionally, since we add more penalty when the model predicts 0, there are plenty of class 0 being misclassified which leads to accuracy decreasing to 66.51 as well as decreasing in specifity. However, in our case, recall should be the first priority as we do not want to lose any potential customers, we should accept this trade off. It is also worth noting that another downside is that the process for training model is more time consuming than before.

\vspace{5mm}
\begin{tabular}{| l | l | l |}
	\hline
	& Really True & Really False \\ \hline
	Predicted True & 558 & 33488 \\ \hline
	Predicted False & 0 & 65954\\
	\hline
\end{tabular}
\vspace{5mm}

\includegraphics[scale = 0.8]{ROCwithandwithoutweights.png}

\subsection{Author contributions}
\begin{enumerate}
	\item
	Data processing:Aoran Zhang, Huiyu Bi
	\item
	Data analysis and interpretation: Meng Li, Zhongyu Fan, Huiyu Bi
	\item
	Model analysis and interpretation:Aoran Zhang, Meng Li
	\item
	Drafting the article: Aoran Zhang, Meng Li, Huiyu Bi, Zhongyu Fan
	\item
	Critical revision of the article:Aoran Zhang, Zhongyu Fan

\end{enumerate}

\section{Logistic Regression}

\subsection{Defining a sample (features-to-label pair)}

Operating under some ambiguity in the Santander problem description, we resolved to define a sample (features-to-label pair) as the values for any two consecutive months for a single customer (identified by ncodpers). Hereafter, the "date" of a sample refers to the month from which the feature values are drawn, not the month from which the labels (purchases) are drawn.

\subsection{ETL}

Our labels (Y-values) are 24 binary fields corresponding to the Santander products which a customer may have purchased. By comparing the product fields from the data file in a given month to those of the previous month (on an individual-customer basis), we determined when each product was purchased and set the corresponding label to 1 if the purchase took place in the month following the date of the sample, 0 otherwise. (If the purchase took place before the 1.5-year range, it is never included in the label.)

For the feature space (X-values), we did following transformation and feature engineering.

We added a column for each product that holds the number of months since the customer’s first purchase of that product. So the 24 newly-added columns contained the information of the purchase history for each customer before the given month.

The variable "fecha\_alta" was the date in which the customer first acquired a contract with the bank. We replaced it with a variable "memberdays" which holds the difference between this date and the sample date, i.e. "fecha\_dato".

As for categorical features, we replaced them with sets of binary features, as normal. For example, if a given categorical feature had three possible values ${A,B,C}$, we replaced that feature with a binary column for $A$, one for $B$, and one for $C$. Unfortunately, two of our categorical variables had over 100 possible values, so our feature space ballooned, threatening the efficacy of our model and motivating regularization later on.

A certain 27334 rows each had exactly 9 missing values in all the same columns. We eliminated the troublesome rows. About 10\% of the samples were missing $renta$; so we filled the missing values by using averages based on province.

Some features were irrelevant for calculation, such as $tipodom$ (address type), and could be discarded easily. Others were used for organizing data but not for calculating regressions: $nomcodpers$ and $fecha\_dato$. $nom\_prov$ was redundant with $cod\_prov$, so we omitted the former.

\subsection{Limiting the Sample}

The data had become quite large, taking 39GB in memory. We were compelled to make use of a cluster owned by the Engineering Department. In order to turn out results quickly and avoid the cost of backtracking, we limited the size of the data with which we worked by taking a random sample from the samples produced by our ETL. We experimented with samples of different size and eventually proceeded with a working set of 197968 ($1/2^6$ as many as we produced in ETL). We used a random 10\% of these samples as testing data.

\subsection{Regression}

We performed a logistic regression on each of the 24 products that a customer might have purchased. We used the sigmoid function $\frac{1}{1+e^{w^Tx}}$.

We performed logistic regression using an augmented error if $\lambda \times L_2norm$ (ridge regression). Naturally, we would use cross-validation to identify the best $\lambda$ for the data, but due to time constraints, we used our sense of the problem to decide on a $\lambda$ value of 0.1. If we were to redo this, we would perform 10-fold cross-validation over a range of possible $\lambda$'s and choose the one that produced the lowest validation error.

We considered a variable learning rate such as Newton's method or a rate that decayed as the iteration count grew. Ultimately, we used an implementation inspired by steepest descent: we began each weight-update with a learning rate of 0.1, which we supposed would be too large in many cases. If the new weights produced an error larger than the previous one, we would try a learning rate half as large, and proceed from there in the fashion of a binary search for some time until we found a minimum error for the current gradient or until we reached some number of iterations.

\subsection{Results}

For the 24 products (i.e. regressions), we calculated the following mean-squared testing errors on our held-out data:

\begin{table}[h]\footnotesize
	\centering
	\caption{Held-out Data MSE}
	\label{my-label}
	\begin{tabular}{llllllll}
		\hline
		MSE = 0                            & ahor\_fin:  & cder\_fin: & deme\_fin: & hip\_fin:  & viv\_fin:  &                     &            \\
		& 0                    & 0          & 0          & 0          & 0          &                     &            \\ \hline
		0 \textless MSE \textless 0.001      & ctma\_fin:          & ctop\_fin: & deco\_fin: & fond\_fin: & plan\_fin: & reca\_fin:  & valo\_fin: \\
		& 0.000303            & 0.000455   & 0.000253   & 0.000253   & 0.000101   &  0.000707             & 0.000354   \\ \hline
		0.001 \textless MSE \textless 0.01 & cco\_fin:           & cno\_fin:  & ecue\_fin: & dela\_fin: & tjcr\_fin: & nomina:             & nom\_pens: \\
		& 0.006567            & 0.002576   & 0.001667   & 0.001061   & 0.005203   & 0.005607            & 0.006921   \\ \hline
		0.01 \textless MSE \textless 1     & ctju\_fin:          & ctpp\_fin: & recibo:    &            &            &                     &            \\
		& 0.999949            & 0.999899   & 0.011517   &            &            &                     &            \\ \hline
		MSE = 1                            & aval\_fin:          & pres\_fin: &            &            &            &                     &\\
		&                    & 1          &1           &            &            &                     & \\
		\hline
	\end{tabular}
\end{table}
The mean of these errors is 0.1685.

\subsection{Discussion}

The results of the logistic regressions are unfortunately suspect. A quick examination shows that the gamut of possible errors is crossed completely: testing errors range from 0 to 1, inclusive. More than likely, the frequency of errors at these two extremes may be explained by the scarcity of purchase data for some products. Even considering all of the Santander data that were supplied, some purchases are so rare that it is entirely conceivable that our random sample contains zero, one, or only a handful of 1's in the corresponding label column. In the event that no 1's are to be found for a given product, it is no wonder that our testing error would be 0; no other error is possible.

Our limitation of the dataset to $1/2^6$ of its original size certainly introduces room for error, but even so, we have almost 200K samples to work with and only 419 features. This is well past the ten-times-the-VC-dimension rule of thumb.\cite{amlbook}

Perhaps a greater amount of feature reduction should have been performed. On a typical example from our 24 regressions, 251 non-zero weights remained. However, only 19 weights measured greater than 0.00001. We intended to perform PCA in order to combat the explosion of features that the bugaboo categorical features imposed: $pais\_residencia$, $canal\_entrada$, and $cod\_prov$ each gave rise to a great many binary features, thereby heightening risk of disproportionately weighting the original categorical columns.

Our decisions for how to fill in missing values also likely contribute to some inaccuracy in our model.

\subsection{Author contributions}
\begin{itemize}
	\item Bowen He:  Worked on data analysis and interpretation, wrote the code to create purchase history variables, worked on feature engineering, combined and revised the discussion part for all groups.

	\item Yuan Tian:  Worked on data analysis and interpretation, created the data cleaning code, generated the idea of ETL process, i.e. defining labels and feature space, and revised the report.

	\item Miao Wang:  Worked on data analysis and interpretation, wrote the code to create response variables, worked on feature engineering, and revised the report.

	\item Markham Anderson: Wrote the subgroup report, worked on data processing, created the Logistic regression model, wrote code to get the classifier and evaluated its performance.
\end{itemize}

\newpage

\begin{appendices}

	\section{Random Forest Data Processing}
	\label{appendix:rn_data}
	\begin{longtable}{p{3cm}|p{5cm}|p{5cm}}
		Column Name & Description & How To Flatten  \\
		\hline
		fecha\_dato & The table is partitioned for this column & Remove in flattening \\
		ncodpers & Customer code & Keep as key in flattening \\
		ind\_empleado & Employee index: A active, B ex employed, F filial, N not employee, P pasive & Drop, since there are 13,576,218 NAs which is a huge proportion in the whole data set \\
		pais\_residencia & Customer's Country residence & Use mode to fill NAs and flattening\\
		sexo & Customer's sex & Use mode to clean the data and flattening\\
		age & Age & Use max/final age for each customer. Outliers were moved to the median according to different age range \\
		fecha\_alta & The date in which the customer became as the first holder of a contract in the bank & Shouldn't change \\
		ind\_nuevo & New customer Index. 1 if the customer registered in the last 6 months. & Drop, since we have fecha\_alta \\
		antiguedad & Customer seniority (in months) & Drop, since we have fecha\_alta \\
		indrel & 1 (First/Primary), 99 (Primary customer during the month but not at the end of the month) & Drop, since we have indrel\_1mes \\
		ult\_fec\_cli\_1t & Last date as primary customer (if he isn't at the end of the month) & Drop, since we have indrel\_1mes \\
		indrel\_1mes & Customer type at the beginning of the month ,1 (First/Primary customer), 2 (co-owner ),P (Potential),3 (former primary), 4(former co-owner) & Use mode \\
		tiprel\_1mes & Customer relation type at the beginning of the month, A (active), I (inactive), P (former customer),R (Potential) & Drop, since we have indrel\_1mes, activity index \\
		indresi & Residence index (S (Yes) or N (No) if the residence country is the same than the bank country) & Drop, since we have pais\_residencia \\
		indext & Foreigner index (S (Yes) or N (No) if the customer's birth country is different than the bank country) & Drop, since we have pais\_residencia \\
		conyuemp & Spouse index. 1 if the customer is spouse of an employee & Drop, since too many NA \\
		canal\_entrada & channel used by the customer to join & Drop, since too many NA \\
		indfall & Deceased index. N/S & Drop, since we are dropping deceased custos \\
		tipodom & Addres type. 1, primary address & Drop, we assume it won't matter \\
		cod\_prov & Province code (customer's address) & Use mode \\
		nomprov & Province name & Drop, since we have cod\_prov \\
		ind\_actividad\_cliente & Activity index (1, active customer; 0, inactive customer) & Length of most recent inactive period \& how long ago that period was \\
		renta & Gross income of the household & Use mean, fill NAs by median household income of different province \\
		segmento & segmentation: 01 - VIP, 02 - Individuals 03 - college graduated & Use mode \& last \\
		ind\_ahor\_fin\_ult1 & Saving Account & Timing: Months from fecha\_alta until first fecha\_dato in which product is purchased \& Length: Use sum \\


	\end{longtable}

\end{appendices}

\begin{thebibliography}{9}

	\bibitem{xg1}
	Tianqi Chen, Carlos Guestrin
	\emph{XGBoost: A Scalable Tree Boosting System}. Preprint.

	\bibitem{xg2}
	\emph{Awesome XGBoost}.
	https://github.com/dmlc/xgboost/tree/master/demo

	\bibitem{xg3}
	\emph{XGBoost Parameters}.
	http://xgboost.readthedocs.io/en/latest//parameter.html

	\bibitem{rnLeo}
	Breiman, Leo and Cutler, Adele
	\textit{Random Forest}.
	\\\texttt{https://www.stat.berkeley.edu/~breiman/RandomForests/cc\_home.htm}

	\bibitem{rnSKlearn}
	Scikit Learn,
	\textit{Ensemble Methods}.
	\\\texttt{http://scikit-learn.org/stable/modules/ensemble.html}

	\bibitem{book1}
	Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, \textit{An Introduction to Statistical Learning}. New York, USA: Springer, 2013.

	\bibitem{book2}
	Trevor Hastie, Robert Tibshirani, Jerome Friedman, \textit{The Elements of Statistical Learning}. New York, USA: Springer, 2008.

	\bibitem{Amlbook}
	Yaser S. Abu-Mosta, Malik Magdon-Ismail, Hsuan-Tien Lin, \textit{Learning From Data: A Short Course}. New York, USA: AMLBook, 2012.

	\bibitem{book}Kevin, P. Murphy, \textit{Machine Learning: A Probabilistic Perspective}. London, England:The MIT Press, Cambridge Massachusetts, 2012.

	\bibitem{paper1}
	herkassky, Vladimir, and Yunqian Ma, \textit{Practical selection of SVM parameters and noise estimation for SVM regression}. Neural networks 17.1 (2004): 113-126.

	\bibitem{paper2}
	Yehuda Koren, Robert Bell, and  Chris Volinsky,
	\textit{Matrix Factorization Techniques for Recommender Systems}. IEEE Computer Society, 2009.
\end{thebibliography}

\end{large}
\end{spacing}
\end{document}
